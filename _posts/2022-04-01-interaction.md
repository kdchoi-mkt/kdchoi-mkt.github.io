---
title: "변수간 상호작용 캐치하기"
categories:
  - data-analysis
tags:
  - 일반 데이터 분석론
  - 통계
toc: true
sidebar:
  nav: data
use_math: true
---

# 들어가기 앞서...

이번 내용을 이해하기 위해서는 `linear regression`에 대한 이해가 필요합니다.

# 두 개의 변수가 상호작용 하는데?

우리에게 설명 변수 $A$와 설명 변수 $B$가 있다고 해봅시다. 이를 통해 우리는 $Y$값을 예측하고, 얼마나 $A$, $B$가 $Y$를 잘 설명하는지 보고 싶습니다. 이럴 때 보통 쓰는건 linear regression 등의 계수를 측정하는 statistical model입니다. 그런데 $A$와 $B$가 각각 $Y$에 관여하는게 아니라, 가끔씩은 “동시에 관여"하기도 합니다. 예를 들어서 우리가 데이터를 쪼개봤더니

1. $A$가 10일때는 $B$가 30만큼 $Y$에 영향을 미치고
2. $A$가 20일때는 $B$가 50만큼 $Y$에 영향을 미친다는 결과가 나왔습니다

이런 식으로 $A$와 $B$가 상호작용을 해서 coefficient가 달라지는 효과를 어떻게 잡을 수 있을까여?

# 문제를 수식으로 만들기

문제를 간단하게 하기 위해 우리가 수학적으로 잘 알고있는 형태인 linear regression에서 다시 이야기해봅시자. 우리가 일반적으로 linear regression을 $Y$에 대해 시행하면 다음과 같은 식의 계수를 얻을 수 있습니다.

$$
Y = \beta_aA+ \beta_bB
$$

그런데 중요한건, 여기서 저 $\beta_b$가 $A$에 대해서 상수가 아니라는 것이 문제입니다. 다른 말로 하면,

$$
\beta_b := \beta_b(A)
$$

의 꼴로 주어지며, 조금 더 정확히는

$$
\beta_b(A) = 10 + 2A
$$

의 꼴을 가진다는 것입니다. 이걸 기존의 $Y$에 대입을 하면, 우리는 다음과 같은 듣도보도못한 새로운 식을 얻을 수 있습니다.

$$
Y = \beta_a A + 10B + 2A\times B
$$

물론 이 방식의 가장 큰 문제는 실제로 regression을 취했을 때 $\beta_a$와 $\beta_b$는 constant하게 추정된다는 점이긴 합니다. 아무튼, conceptually하게 문제를 풀었더니 $A$와 $B$의 곱의 형태가 새로 튀어나온걸 확인할 수 있었습니다. 그렇다면 저러한 $A\times B$가 상호작용을 나타내주는 변수가 아닐까요?

# 상호작용은 곱하기

어떤 좋은 예시 하나를 통해서 우리는 곱하기 텀이 상호작용을 나타내는 변수가 아닌지 의심을 할 수 있게 되었습니다. 그렇다면, 이게 정말로 상호작용을 나타내는지 한 번 생각을 해봅시자. 이제 우리에겐 세가지 변수가 있고, 각각의 coefficient를 측정할 수 있습니다

$$
Y = \beta_aA + \beta_bB + \beta_iA\times B
$$

여기서 각 $\beta$들은 non-zero coefficient라고 합시다. 만약 $A$가 10으로 고정되어있으면, $B$의 $Y$에 미치는 영향은 $\beta_b + 10\beta_i$가 될 것입니다. 반대로, $B$가 -100으로 고정되어있으면 $A$의 $Y$에 미치는 영향은 $\beta_a - 100\beta_i$가 될 것입니다. 이를 통해 $A$의 값에 따라서 $B$가 $Y$에 주는 영향, $B$의 값에 따라서 $A$가 $Y$에 주는 영향 모두가 달라지는걸 확인할 수 있고, 이런 non-linear interaction을 $\beta_i$로 잡아낼 수 있다는 뜻입니다. 당연히, $A$의 값에 따라서 $B$가 주는 영향이 아예 사라지거나 엄청 증폭되기 때문에 이러한 interaction term의 coefficient를 `조절 효과 (moderation effect)`로도 해석합니다.

물론 더 나아가서 $A^2B, AB^3, log(A + B)$ 등의 non-linear function은 대부분 interaction을 잡아낼 수 있습니다. 그러나 우리가 $A\times B$를 interaction term으로 사용하는 가장 큰 이유는 만들기 단순하고 직관적이며, 해석도 쉽기 때문입니다.

# 결론

우리가 살아가면서 관측하는 대부분의 변수는 독립적으로 종속 변수에 영향을 주지 않습니다. 항상 어떠한 marginal effect가 상호간에 적용되면서 복잡한 형태로 종속 변수에 영향을 줍니다. 예를 들어서, 소비자가 음료수를 구매를 한다고 했을 때 그 상품의 브랜드에 따라 가격에 대한 민감도가 다른 것 처럼 말입니다. 사실 이번 포스팅에서는 단순히 이러한 interaction effect가 linear하게 일어난다고 이야기했습니다. 그러나 실제로는 선형적인 interaction이 일어나는 것 자체가 너무 큰 가정입니다. 그렇기에 어떠한 수량을 예측하거나 분류를 예측할 때 `Deep Learning`의 방법을 사용하는게 더욱 예측을 간단하고 강력하게 할 수 있습니다. 그렇지만, DL의 방법을 써버리는 순간, 우리는 이제 더이상 `explainability를 사용할 수 없습니다`. 그리고 $X$ 변수가 들어간 non-linear function에 대해서는 웬만하면 선형 가정을 했을때에도 fit이 어느정도 잘 들어맞는 것 같기에, 정확한 예측이 아니라 추세를 본다는 기준에서는 해석이 가능한 linear interaction을 사용하는 것 같습니다.

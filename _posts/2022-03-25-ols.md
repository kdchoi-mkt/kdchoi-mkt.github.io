---
title: "Ordinary Least Square의 의미"
categories:
  - data-analysis
tags:
  - 수리과학
  - 선형대수
toc: true
sidebar:
  nav: data
use_math: true
---

# 들어가기 앞서...

경영학의 관점에서 머신러닝, 또는 통계 영역에서 여러가지 모델을 사용하는 이유는 X 변수가 y 변수에 얼마나 영향을 주는지 알고, 그것을 개선하거나 새로운 액션을 취하기 위함입니다. 실제로 영향을 주는지 알려주는 모델은 다양하게 있는데, 그 중 설명하기 쉬운 모델은 선형 모델입니다. 간단히 X가 1만큼 증가하면 y가 얼마나 많이 증가하는지 알려주기 때문이죠. 그러면 선형 모델은 어떻게 만들어지고, 이게 수학적으로는 어떤 의미가 있을까요? 이번 포스팅에서는 최소제곱법을 이용한 선형 모델(Ordinary least square; OLS)을 방정식으로 풀어보고, 이것의 수학적인 의미도 같이 알아보도록 하겠습니다.

이번 포스팅을 이해하기 위해서는 `선형 대수(Linear Algebra)`에 대한 지식이 필요합니다.

# 방정식으로서 OLS

우선, 전통적으로 OLS를 구하는 방법을 알아보겠습니다. 우리가 원하는 식은 다음과 같습니다.

$$
\begin{align*}
y&=X\beta \\
&\text{ for }  y\in\mathbb{R}^{n\times 1}, \beta \in\mathbb{R}^{m\times 1}, \\
X&=\begin{bmatrix}
x_1 & x_2 & \cdots & x_m
\end{bmatrix}
\in\mathbb{R}^{n\times m}
\end{align*}
$$

여기서 $y$는 종속 변수, $X$는 독립 변수로, 단순히 말하자면 $y$를 설명하기 위해 만든 설명 변수입니다. $\beta$는 각 $X$의 열벡터 $x_{i}$가 $y$에 얼마나 영향을 주는지 알려주는 변수입니다. 그러면 $\beta$를 어떻게 잘 추정할 수 있을까요? 다른 말로 하면, 주어진 데이터를 통해서 가장 최적의 $\beta$는 어떤 성질을 띄고 있을까요? 그 중 가장 많이 쓰이는 방법이 $y-X\beta$의 값이 최소가 되게 하는 것입니다. 머신러닝을 공부하신 분들이라면 다음의 식을 많이 보셨을 겁니다.

$$
\operatorname{argmin}_{\beta}\sum_{i=1}^n\left\{y_i-\sum_{j=l}^m x_{ij}\beta_j\right\}
$$

이는 Linear Regression의 cost function으로도 많이 쓰이며, ridge나 lasso의 경우에는 저 옆에 $L_p$ norm이 추가되기도 합니다. 편의상 $y-X\beta=e$로 둔다면 다음과 같이 적을 수 있습니다.

$$
\operatorname{argmin}_{\beta}\|y-X\beta\|_2^2=\operatorname{argmin}_{\beta}\|e\|_2^2
$$

여기서 $\|v\|_2=\sqrt{\sum v_i^2}$인 matrix norm이며, $e$를 residual, 혹은 잔차라고 부릅니다. 이런식으로 $e$의 제곱의 합이 최소가 되도록 $\beta$를 추정하기 때문에 이러한 메소드를 최소 제곱법이라고도 불립니다. 일반적으로 최솟값, 혹은 최댓값을 구하는 최적화 기법은 explicit하게 해가 표현될 수 있는 analytic solution이 없습니다. 그러나, 특정 조건만 잘 만족한다면 선형 회귀 한정으로 해를 구할 수 있습니다. 만약 $X$의 열벡터가 모두 선형 독립이고, $X$와 $e$가 uncorrelated 되어있다면,

$$
\begin{align*}
y&=X\beta+e\\
X^Ty&=X^TX\beta+X^Te=X^TX\beta \\
\beta &= (X^TX)^{-1}X^Ty
\end{align*}
$$

로 $\beta$를 구할 수 있는 것이죠. 그러나 이 식을 도출할 때에는 $e$에 대한 성질을 건드리지 않습니다. 그러면 정말로 이 식이 $e$를 최소로 만드는지는 다음 장에서 알아보도록 하겠습니다. 다만, 우회적으로 이 식이 $e$를 최소화하는걸 Gauss-Markov 정리로 보일 수 있습니다. 이것까지 다루게 되면 내용이 방대해지므로 이는 다음에 다루도록 하겠습니다.

# Matrix Operator로서 OLS

저번 PCA 포스팅에서도 언급했듯이, 모든 행렬은 선형 함수로 작용할 수 있습니다. 이 컨셉을 통해서 위에서 도출한 $\beta=(X^TX)^{-1}X^Ty$의 의미를 하나하나 살펴보도록 하겠습니다.

우선, $X^Ty$가 뭘까요? 이를 matrix-wise로 풀어보면 다음과 같이 볼 수 있습니다.

$$
\begin{align*}
X^Ty&=\begin{bmatrix}
-x_1- \\
\vdots \\
-x_m-
\end{bmatrix}
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix}\\
&=\begin{bmatrix}
x_1^Ty \\
\vdots \\
x_m^Ty
\end{bmatrix}
=\begin{bmatrix}
\langle x_1, y\rangle\\
\vdots \\
\langle x_m, y\rangle
\end{bmatrix}
\end{align*}
$$

$X^T$의 함수적인 성질은 $y$를 $x_i$에 대한 내적을 의미하는군요! ($x_i\in\mathbb{R}^n$이라는 사실에 주의하세요!) 이를 다르게 말하면 $y$를 $\text{col}(X)=\text{span}\{x_i\}$위로 변환 시켰다는 의미기도 합니다. 우리가 모든 변수를 다 만들어서 $y$를 100% 다 설명시킬 수 있으면 좋겠지만, 일반적인 경우에는 그렇지 않기 때문에 $y$를 $\text{span}\{x_i\}$위로 변환을 시킨 뒤, 관계를 보는 것입니다.

자 그러면, $(X^TX)^{-1}$는 어떤 작용을 할까요? 이를 알기 위해서는 $X^TX$가 벡터를 어떻게 이동시키는지 관찰하기만 하면 충분합니다. $X^TX$를 직접 적어보면 다음과 같이 적을 수 있습니다.

$$
\begin{align*}
X^TX&=\begin{bmatrix}
-x_1- \\
\vdots \\
-x_m-
\end{bmatrix}
\begin{bmatrix}
| & & |\\
x_1 & \cdots & x_m \\
| && |
\end{bmatrix} \\
&=
\begin{bmatrix}
\langle x_1, x_1 \rangle & \cdots & \langle x_1, x_m \rangle \\
\vdots & \cdots & \vdots \\
\langle x_m, x_1 \rangle & \cdots & \langle x_m, x_m \rangle
\end{bmatrix}
\end{align*}
$$

$X^TX$는 $x_i, x_j$의 내적값을 엔트리로 가지고 있는 matrix였군요. 그러면 여기에 벡터를 곱하면 다음과 같이 계산됩니다.

$$
\begin{align*}
(X^TX)c&=
\begin{bmatrix}
\langle x_1, x_1 \rangle & \cdots & \langle x_1, x_m \rangle \\
\vdots & \cdots & \vdots \\
\langle x_m, x_1 \rangle & \cdots & \langle x_m, x_m \rangle
\end{bmatrix}\begin{bmatrix}
c_1\\
\vdots\\
c_m
\end{bmatrix}\\
&=\begin{bmatrix}
\langle x_1, \sum{c_ix_i} \rangle \\
\vdots \\

\langle x_n, \sum{c_ix_i} \rangle
\end{bmatrix}
\end{align*}
$$

여기서 알 수 있는 사실은 $c$가 사실은 $\text{span}\{x_i\}$위의 벡터라는 점이며, $(X^TX)$는 $x_i$에 대한 $c$의 내적 벡터를 계산하는 operator라는 점이죠. 그렇다면 $(X^TX)^{-1}$이 받는 벡터는 내적 벡터이고, 이를 통해 $\text{span}\{x_i\}$위의 적절한 벡터로 원복시키는 함수라는 것을 알 수 있습니다! 그런데 이렇게 생긴 행렬... 어디서 보지 않았나요? 이전 PCA 포스팅에서의 $Cov(X, X)$와 같군요! 사실 어떻게 보면 $Cov(x_i, x_j)$ 역시 내적으로 생각할 수 있기 때문에, 이 행렬과 같은 선상 위에 있다고 볼 수 있습니다. 여담으로, 이렇게 생긴 행렬은 특이하기 때문에 따로 부르는 이름이 있습니다. Gram Matrix라고 부르는데, 여러가지 좋은 특징이 있으니 관심있으신 분들은 찾아보는걸 추천드립니다. 당장 두번의 포스팅에 알게 모르게 나왔잖아요?

따라서, $(X^TX)^{-1}X^Ty$의 의미는, 먼저 $y$를 $\text{col}(X)$위로 변환시킨 후($X^T)$, 이 값이 나오게 하는 적절한 $\beta$를 계산하는 것($(X^TX)^{-1}$)을 의미합니다. 그런데 아직까지 풀리지 않는 의문점이 있습니다. 과연 이렇게 만든 $\beta$가 정말 $e$를 최소로 만들어 주는가? 이를 해결하기 위해서는 한 단계 스텝을 더 진행해야합니다.

# Operator Theory를 통한 OLS

과연 사영이 무엇일까요? 어떤 함수가 잘 있습니다. 이 함수는 어떤 성질을 가져야 사영이라고 부를 수 있을까요? 벡터 $v$를 $P$라는 operator를 통해 사영시킨다고 했을 때, $Pv$를 다시 사영시켜봤자 값이 바뀌면 안되겠죠? 따라서 operator theory에서는 $P^2=P$인 linear operator를 projection operator라고 정의합니다. 조금 더 정확한 statement는 다음과 같습니다.

$$
\begin{align*}
&\text{For }P: V/K\rightarrow V/K \\
&P\text{ is called projection operator if }\\
&P^2v=PPv=Pv
\end{align*}
$$

이 때 $V=P(V)\oplus P(V)^\perp$로 나눌 수 있는데, 중요하나 증명이 길기 때문에 넘어가도록 하겠습니다. 더 중요한것은, $P:V\rightarrow P(V)$인 $P$가 unique하게 존재한다는 것입니다. 무슨 말이냐고 하면, $V$의 subspace $W$로 projection하는 operator는 unique하다는 것이죠. 자 그러면 다시 우리의 그 문제의 방정식을 들고와봅시다.

$$
\begin{align*}
y&=X\beta+e \\
y&=X(X^TX)^{-1}X^T+e
\end{align*}
$$

이렇게 바꿀 수 있는데,

$$
\begin{align*}
(X(X^TX)^{-1}X^T)^2&=X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T\\
&=X(X^TX)^{-1}X^T
\end{align*}
$$

즉, $X(X^TX)^{-1}X^T$는 $y$를 $\text{col}(X)$로 사영을 내리는 operator이며, 따라서 $e$는 $\text{col}(X)$에 수직이고 그것이 $e$를 최소로 만들어주는 것임을 긴 차례에 걸쳐 증명했습니다.

# 결론

사실 이런것들은 몰라도 됩니다. 웬만한 경우에서 OLS를 쓰는 이유가 residual을 최소화하기 위해서라는 것만 알아도 충분하기 때문입니다. 그러나, 이런 선대적인 직관을 키우는 것은 계산 과정이나 결과에 대한 해석을 하는데 꽤나 중요한 역할을 하기도 합니다. 이번 포스팅을 통해 직관에 대해서 트레이닝을 했다면, 다음 포스팅에서는 interaction이 어떤 역할을 하는지에 대해서 알아보도록 하겠습니다.
